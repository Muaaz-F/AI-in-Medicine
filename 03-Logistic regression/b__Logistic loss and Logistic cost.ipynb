{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644c3fe8",
   "metadata": {},
   "source": [
    "## Logistic Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed52a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522dc127-31a4-4ae2-a371-e7aaaa35778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
    "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6387b",
   "metadata": {},
   "source": [
    "## Squared error for logistic regression?\n",
    "<img align=\"left\" src=\"./images/C1_W3_SqErrorVsLogistic.png\"     style=\" width:400px; padding: 10px; \" > Recall for **Linear** Regression we have used the **squared error cost function**:\n",
    "The equation for the squared error cost with one variable is:\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "where \n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04346b9a",
   "metadata": {},
   "source": [
    "Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32071bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bowl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c364c52",
   "metadata": {},
   "source": [
    "This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, $f_{wb}(x)$ now has a non-linear component, the sigmoid function:   $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$.   Let's try a squared error cost on the example from an earlier lab, now including the sigmoid.\n",
    "\n",
    "Here is our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
    "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
    "plt_simple_example(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45c7aa",
   "metadata": {},
   "source": [
    "Now, let's get a surface plot of the cost using a *squared error cost*:\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ \n",
    " \n",
    "where \n",
    "  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76082bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt_logistic_squared_error(x_train,y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0818de",
   "metadata": {},
   "source": [
    "While this produces a pretty interesting plot, the surface above not nearly as smooth as the 'soup bowl' from linear regression!    \n",
    "\n",
    "Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19affd0",
   "metadata": {},
   "source": [
    "## Logistic Loss Function\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_a.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_b.png\"     style=\" width:250px; padding: 2px; \" >\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticLoss_c.png\"     style=\" width:250px; padding: 2px; \" > "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab752845",
   "metadata": {},
   "source": [
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_two_logistic_loss_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d243b",
   "metadata": {},
   "source": [
    "Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is $f_{\\mathbf{w},b}$ which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063efadc",
   "metadata": {},
   "source": [
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4deb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd26131",
   "metadata": {},
   "source": [
    "This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline. Reminder: you can rotate the above plots using your mouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18617d55",
   "metadata": {},
   "source": [
    "Thus, we determined a squared error loss function is not suitable for classification tasks. Also, we developed and examined the logistic loss function which **is** suitable for classification tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec276dd8-144e-4c0c-947a-3dab5d72e320",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b8178-b37d-439d-829b-f5d97596042d",
   "metadata": {},
   "source": [
    "### Cost Function for Logistic Regression\n",
    "Now, we examine the implementation and utilize the cost function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447d8869-ef8a-4510-801f-0d24778b3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import  plot_data, sigmoid, dlc\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a45e57-30a2-4110-9a72-8d19bb11b76a",
   "metadata": {},
   "source": [
    "Let's start with the same dataset as was used in the decision boundary lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30968015-64e4-4a97-a59b-e62289636b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ffab7-ddc0-4155-a134-53f4b21cdd62",
   "metadata": {},
   "source": [
    "We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17726e-fb50-4a99-9aac-0c2cbb9c1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "# Set both axes to be from 0-4\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cd791-d6e6-48d2-bdab-f721e0fe3e33",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4822d1-83c4-4eea-a2aa-5fa321ffffe1",
   "metadata": {},
   "source": [
    "<a name='ex-02'></a>\n",
    "#### Code Description\n",
    "\n",
    "The algorithm for `compute_cost_logistic` loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "\n",
    "Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($𝑚$,) respectively, where  $𝑛$ is the number of features and $𝑚$ is the number of training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2a5f7d-9137-47a4-8057-7f6e6d71ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a1a09-06fe-43aa-867f-e9b84a138554",
   "metadata": {},
   "source": [
    "Check the implementation of the cost function using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8fac0-9803-4e85-ad17-b88a2c0c9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d039dc3-322c-411f-bbd2-fe64caa4a3d2",
   "metadata": {},
   "source": [
    "**Expected output**: 0.3668667864055175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc0f95-e572-4fc2-803d-c21ae0c81226",
   "metadata": {},
   "source": [
    "### Example\n",
    "Now, let's see what the cost function output is for a different value of $w$. \n",
    "\n",
    "* In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had `b = -3, w = np.array([1,1])`.\n",
    "\n",
    "* Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or `b = -4, w = np.array([1,1])` provides a better model.\n",
    "\n",
    "Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.\n",
    "\n",
    "* For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)\n",
    "* For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244ff85-acc2-4193-8514-f9500270bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose values between 0 and 6\n",
    "x0 = np.arange(0,6)\n",
    "\n",
    "# Plot the two decision boundaries\n",
    "x1 = 3 - x0\n",
    "x1_other = 4 - x0\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "# Plot the decision boundary\n",
    "ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\n",
    "ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\n",
    "ax.axis([0, 4, 0, 4])\n",
    "\n",
    "# Plot the original data\n",
    "plot_data(X_train,y_train,ax)\n",
    "ax.axis([0, 4, 0, 4])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46540d54-bb05-4f46-9fd8-0bdbba40950e",
   "metadata": {},
   "source": [
    "You can see from this plot that `b = -4, w = np.array([1,1])` is a worse model for the training data. Let's see if the cost function implementation reflects this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5eca2-70fc-455c-8a9b-ab1068aa35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array1 = np.array([1,1])\n",
    "b_1 = -3\n",
    "w_array2 = np.array([1,1])\n",
    "b_2 = -4\n",
    "\n",
    "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
    "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97799780-cf93-4a28-a423-8248302e2671",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "<br>Cost for b = -3 :  0.3668667864055175\n",
    "<br>Cost for b = -4 :  0.5036808636748461\n",
    "<br>You can see the cost function behaves as expected and the cost for `b = -4, w = np.array([1,1])` is indeed higher than the cost for `b = -3, w = np.array([1,1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26eda81-c295-4c35-9635-27b80ebf4af6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
